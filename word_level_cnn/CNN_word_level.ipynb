{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_ = '/home/qingyi/data/yelp/yelp_'\n",
    "dataset =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('dataset', dataset)\n",
    "train_y = []\n",
    "train_x = []\n",
    "with open(path_ + 'train' + str(dataset) + '.json') as f:\n",
    "    for line in f:\n",
    "        t = json.loads(line)\n",
    "        train_y.append(t['label'])\n",
    "        train_x.append(t['text'])\n",
    "train_x,train_y = shuffle(train_x,train_y)\n",
    "\n",
    "test_y = []\n",
    "test_x = []\n",
    "\n",
    "with open(path_ + 'test' + str(dataset) + '.json') as f:\n",
    "    for line in f:\n",
    "        t = json.loads(line)\n",
    "        test_y.append(t['label'])\n",
    "        test_x.append(t['text'])\n",
    "test_x,test_y= shuffle(test_x,test_y)\n",
    "print 'loaded properly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_split_length(data):\n",
    "    return len(clean_str(data).split())\n",
    "def clean_split(data):\n",
    "    return clean_str(data).split()\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_each(data):\n",
    "    data = clean_split(data)\n",
    "    for word in data:\n",
    "        if word in count:\n",
    "            count[word]+=1\n",
    "        else:\n",
    "            count[word]=1\n",
    "\n",
    "tick = time.time()\n",
    "count = {}\n",
    "instance =0\n",
    "for x in train_x:\n",
    "    count_each(x)\n",
    "    instance +=1 \n",
    "    if instance%100000==0:\n",
    "        print 'instance processed '+ str(instance)\n",
    "tock= time.time()\n",
    "print str((tock-tick)/60) + 'minutes to process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computed in full file\n",
    "num_class= 2 # number of differnt classes\n",
    "batch_size = 128\n",
    "embedding_size=300\n",
    "vocab_size =len(dictionary)\n",
    "n_train= len(train_x)\n",
    "num_epoch=2\n",
    "max_sentence_length =500\n",
    "\n",
    "print ('vocab size', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorizer(vec,size):\n",
    "    vector =[]\n",
    "    for each in vec:\n",
    "        v= np.zeros(size)\n",
    "        v[each]=1\n",
    "        vector.append(v)\n",
    "    return vector\n",
    "train_y= vectorizer(train_y,num_class)\n",
    "test_y=vectorizer(test_y,num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# based on ideas from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "x = tf.placeholder(tf.int32, shape =[None, max_sentence_length])\n",
    "y_ =tf.placeholder(tf.float32, shape = [None,num_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess= tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #building a multilayer convolutional network\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "#Convolution and Pooling\n",
    "def conv2d(x, W,Strides = [1, 1, 1, 1]):\n",
    "    return tf.nn.conv2d(x, W, strides=Strides, padding='VALID')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x, k_size=[1, 1, 1, 1], Strides=[1, 1, 1, 1]):\n",
    "    return tf.nn.max_pool(x, ksize=k_size,\n",
    "                          strides=Strides, padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('embeddings'):\n",
    "    embeddings= tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\n",
    "    x_embed_tensor= tf.nn.embedding_lookup(embeddings,x)\n",
    "    x_embed= tf.expand_dims(x_embed_tensor,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First convolutional layer \n",
    "#parameter\n",
    "pooled_outputs=[]\n",
    "filter_sizes =[2,3,4]\n",
    "num_filters = 50\n",
    "s= 3 # hyperparemeter for norm scalings\n",
    "with tf.name_scope(\"conv-layer\"):\n",
    "    W_shape = [filter_sizes[0],embedding_size,1, num_filters]\n",
    "    W_2 = weight_variable(W_shape) \n",
    "    b = bias_variable([num_filters])\n",
    "    conv =  conv2d(x_embed, W_2) + b\n",
    "    h_conv_2= tf.nn.relu(conv)\n",
    "    pool = max_pool_2x2(h_conv_2, [1, max_sentence_length-filter_sizes[0]+1, 1, 1 ])\n",
    "    pooled_outputs.append(pool)\n",
    "    \n",
    "    W_shape = [filter_sizes[1],embedding_size,1, num_filters]\n",
    "    W_3 = weight_variable(W_shape) \n",
    "    b = bias_variable([num_filters])\n",
    "    conv =  conv2d(x_embed, W_3) + b\n",
    "    h_conv_3 = tf.nn.relu(conv)\n",
    "    pool = max_pool_2x2(h_conv_3, [1, max_sentence_length-filter_sizes[1]+1, 1, 1 ])\n",
    "    pooled_outputs.append(pool)\n",
    "        \n",
    "    W_shape = [filter_sizes[2],embedding_size,1, num_filters]\n",
    "    W_4 = weight_variable(W_shape) \n",
    "    b = bias_variable([num_filters])\n",
    "    conv =  conv2d(x_embed, W_4) + b\n",
    "    h_conv_4 = tf.nn.relu(conv)\n",
    "    pool = max_pool_2x2(h_conv_4, [1, max_sentence_length-filter_sizes[2]+1, 1, 1 ])\n",
    "    pooled_outputs.append(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_filter_total= num_filters*len(filter_sizes)\n",
    "h_pool=tf.concat(3,pooled_outputs)\n",
    "h_flat= tf.reshape(h_pool,[-1, num_filter_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_num_class1_drop = tf.nn.dropout(h_flat, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2_loss = tf.constant(0.0)\n",
    "with tf.name_scope(\"output\"):\n",
    "    W= tf.get_variable(\n",
    "        \"W\",\n",
    "        shape = [num_filter_total, num_class],\n",
    "        initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b= tf.Variable(tf.constant(0.1, shape=[num_class], name =\"b\"))\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores =tf.nn.xw_plus_b(h_num_class1_drop,W,b, name =\"scores\")\n",
    "    predictions= tf.argmax(scores, 1, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the mean cross entropy \n",
    "with tf.name_scope(\"loss\"):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(scores,y_)\n",
    "    loss = tf.reduce_mean(losses) + s * l2_loss\n",
    "    ce_sum = tf.scalar_summary(\"cross entropy (loss)\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    training_summary = tf.scalar_summary(\"training_accuracy\", accuracy)\n",
    "    validation_summary = tf.scalar_summary(\"validation_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training step\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_step= optimizer.apply_gradients(grads_and_vars, global_step= global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(\"/home/qingyi/tensorflow/tensorboard\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_id(w):\n",
    "    if w in dictionary:\n",
    "        return dictionary[w]\n",
    "    return dictionary['UNK']\n",
    "\n",
    "def get_id(x):\n",
    "    x = clean_split(x)\n",
    "    ids = np.full((max_sentence_length), dictionary['NOWORD'],dtype=np.int32)\n",
    "    if len(x)> max_sentence_length:\n",
    "        x= x[-max_sentence_length:]\n",
    "    for i in range(len(x)):\n",
    "        ids[i]=get_word_id(x[i])\n",
    "    #ids =ids.reshape(-1,10)\n",
    "    #return shuffle(ids).flatten()\n",
    "    return ids       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch_id(batch_x):\n",
    "    ids=[]\n",
    "    for x in batch_x:\n",
    "        ids.append(get_id(x))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_acc():\n",
    "    t_len=batch_size*15\n",
    "    test_acc_all=[]\n",
    "    for j in range(len(test_x)/t_len):\n",
    "            batch_test = j*t_len\n",
    "            batch_test_end = batch_test+t_len\n",
    "            batch_test_x= get_batch_id(test_x[batch_test:batch_test_end])\n",
    "            batch_test_y= test_y[batch_test:batch_test_end]\n",
    "            te_ac, test_sum= sess.run([accuracy, validation_summary],feed_dict={x: batch_test_x, y_: batch_test_y, keep_prob: 1.0})\n",
    "            test_acc_all.append(te_ac)\n",
    "    test_accuracy=np.mean(test_acc_all)\n",
    "    print(\"test accuracy %g\" % test_accuracy)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=0\n",
    "train_steps =n_train/batch_size*num_epoch\n",
    "tick = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while s < train_steps:\n",
    "    batch_ind=s*batch_size%n_train\n",
    "    batch_ind_end=batch_ind +batch_size \n",
    "    batch_x = get_batch_id(train_x[batch_ind:batch_ind_end])\n",
    "    batch_y= train_y[batch_ind:batch_ind_end]\n",
    "    \n",
    "    train_step.run(session=sess, feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "    if s% (n_train/batch_size) ==0 and s >0:\n",
    "        print 'after' +str(s/n_train)+ 'epoch'\n",
    "        test_acc()\n",
    "        train_x, train_y=shuffle(train_x,train_y)\n",
    "    if s% 200 ==0:\n",
    "        feed_dict= {x: batch_x, y_: batch_y, keep_prob: 1.0}\n",
    "        summary_str = sess.run(merged, feed_dict= feed_dict)\n",
    "        writer.add_summary(summary_str, s)\n",
    "        train_accuracy, train_sum = sess.run([accuracy, training_summary], feed_dict=feed_dict)\n",
    "        train_loss = loss.eval(session=sess, feed_dict= feed_dict)\n",
    "        writer.add_summary(train_sum, s) \n",
    "        tock= time.time()\n",
    "        test_acc_all=[]\n",
    "        \n",
    "        #evalutate a portion of the test set randomly choosen\n",
    "        t_len=batch_size*10\n",
    "        rg = random.randrange(len(test_y)/(10*t_len))\n",
    "        \n",
    "        for j in range(rg,rg+10):\n",
    "            batch_test = j*t_len\n",
    "            batch_test_end = batch_test+t_len\n",
    "            batch_test_x= get_batch_id(test_x[batch_test:batch_test_end])\n",
    "            batch_test_y= test_y[batch_test:batch_test_end]\n",
    "            te_ac, test_sum= sess.run([accuracy, validation_summary],feed_dict={x: batch_test_x, y_: batch_test_y, keep_prob: 1.0})\n",
    "            test_acc_all.append(te_ac)\n",
    "            \n",
    "        test_accuracy=np.mean(test_acc_all)\n",
    "        print(\"step %d, training accuracy %g, loss %g \" % (s, train_accuracy, train_loss))\n",
    "        print(\"test accuracy %g\" % test_accuracy)\n",
    "        print('time elapsed',(tock-tick)/3600)\n",
    "    \n",
    "    if s % 1000 == 0:\n",
    "        # Append the step number to the checkpoint name:\n",
    "            saver.save(sess, 'yelp_run/my-model', global_step=s)\n",
    "\n",
    "    s+=1\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
